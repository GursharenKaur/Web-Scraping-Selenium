{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1**"
      ],
      "metadata": {
        "id": "yEHhXxLY1Y26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FL88SWgh0Gpv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "# HTTP library for making web requests to download pages\n",
        "#Purpose: HTTP library for making web requests\n",
        "#Function: Downloads web page content from URLs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "# HTML parser to extract data from web pages\n",
        "#Purpose: HTML/XML parsing library\n",
        "#Function: Converts raw HTML into a navigable tree structure"
      ],
      "metadata": {
        "id": "Lx-z0FgO1uXc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Data manipulation library for organizing scraped data\n",
        "#Purpose: Data manipulation and analysis library\n",
        "#Function: Handles structured data operations"
      ],
      "metadata": {
        "id": "aMBZkiTH2FxY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# For adding delays between requests (respectful scraping)"
      ],
      "metadata": {
        "id": "y7IQ_YT72IYn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Regular expressions for text pattern matching and cleaning"
      ],
      "metadata": {
        "id": "s0O8sPUd2Kqe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urljoin, urlparse\n",
        "# URL manipulation utilities"
      ],
      "metadata": {
        "id": "noROTTYJ2LwY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BooksScraper:\n",
        "    def __init__(self, base_url=\"https://books.toscrape.com/\"):\n",
        "        self.base_url = base_url\n",
        "        self.session = requests.Session()\n",
        "        # Add headers to mimic a real browser\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "        self.books_data = []\n",
        "\n",
        "    def get_star_rating(self, star_class):\n",
        "        \"\"\"Convert star rating class to number of stars\"\"\"\n",
        "        rating_map = {\n",
        "            'One': 1,\n",
        "            'Two': 2,\n",
        "            'Three': 3,\n",
        "            'Four': 4,\n",
        "            'Five': 5\n",
        "        }\n",
        "\n",
        "        # Extract the rating from class like \"star-rating Three\"\n",
        "        for rating_word, rating_num in rating_map.items():\n",
        "            if rating_word in star_class:\n",
        "                return rating_num\n",
        "        return 0\n",
        "\n",
        "    def clean_price(self, price_text):\n",
        "        \"\"\"Clean price text and convert to float\"\"\"\n",
        "        # Remove currency symbol and extra spaces\n",
        "        price_cleaned = re.sub(r'[£$€]', '', price_text.strip())\n",
        "        try:\n",
        "            return float(price_cleaned)\n",
        "        except ValueError:\n",
        "            return 0.0\n",
        "\n",
        "    def scrape_book_details(self, book_element):\n",
        "        \"\"\"Extract details from a single book element\"\"\"\n",
        "        try:\n",
        "            # Title\n",
        "            title_element = book_element.find('h3').find('a')\n",
        "            title = title_element.get('title') if title_element else \"N/A\"\n",
        "\n",
        "            # Price\n",
        "            price_element = book_element.find('p', class_='price_color')\n",
        "            price_text = price_element.text if price_element else \"£0.00\"\n",
        "            price = self.clean_price(price_text)\n",
        "\n",
        "            # Availability\n",
        "            availability_element = book_element.find('p', class_='instock availability')\n",
        "            availability_text = availability_element.text.strip() if availability_element else \"Out of stock\"\n",
        "            availability = \"In stock\" if \"In stock\" in availability_text else \"Out of stock\"\n",
        "\n",
        "            # Star Rating\n",
        "            star_element = book_element.find('p', class_=re.compile(r'star-rating'))\n",
        "            if star_element:\n",
        "                star_classes = star_element.get('class', [])\n",
        "                star_rating_class = ' '.join(star_classes)\n",
        "                star_rating = self.get_star_rating(star_rating_class)\n",
        "            else:\n",
        "                star_rating = 0\n",
        "\n",
        "            return {\n",
        "                'Title': title,\n",
        "                'Price': price,\n",
        "                'Availability': availability,\n",
        "                'Star_Rating': star_rating\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting book details: {e}\")\n",
        "            return None\n",
        "\n",
        "    def scrape_page(self, url):\n",
        "        \"\"\"Scrape books from a single page\"\"\"\n",
        "        try:\n",
        "            print(f\"Scraping: {url}\")\n",
        "            response = self.session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Find all book containers\n",
        "            books = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "            page_books = []\n",
        "            for book in books:\n",
        "                book_data = self.scrape_book_details(book)\n",
        "                if book_data:\n",
        "                    page_books.append(book_data)\n",
        "\n",
        "            print(f\"Found {len(page_books)} books on this page\")\n",
        "            return page_books, soup\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error fetching page {url}: {e}\")\n",
        "            return [], None\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing page {url}: {e}\")\n",
        "            return [], None\n",
        "\n",
        "    def find_next_page_url(self, soup, current_url):\n",
        "        \"\"\"Find the URL of the next page\"\"\"\n",
        "        if not soup:\n",
        "            return None\n",
        "\n",
        "        # Look for next page link\n",
        "        next_link = soup.find('li', class_='next')\n",
        "        if next_link:\n",
        "            next_a = next_link.find('a')\n",
        "            if next_a and next_a.get('href'):\n",
        "                # Construct absolute URL\n",
        "                next_relative_url = next_a.get('href')\n",
        "                next_url = urljoin(current_url, next_relative_url)\n",
        "                return next_url\n",
        "\n",
        "        return None\n",
        "\n",
        "    def scrape_all_books(self):\n",
        "        \"\"\"Scrape books from all pages\"\"\"\n",
        "        print(\"Starting to scrape all books from Books to Scrape...\")\n",
        "\n",
        "        current_url = self.base_url\n",
        "        page_number = 1\n",
        "\n",
        "        while current_url:\n",
        "            print(f\"\\n--- Page {page_number} ---\")\n",
        "\n",
        "            # Scrape current page\n",
        "            page_books, soup = self.scrape_page(current_url)\n",
        "\n",
        "            if not page_books:\n",
        "                print(\"No books found on this page, stopping...\")\n",
        "                break\n",
        "\n",
        "            # Add books to our collection\n",
        "            self.books_data.extend(page_books)\n",
        "\n",
        "            # Find next page URL\n",
        "            next_url = self.find_next_page_url(soup, current_url)\n",
        "\n",
        "            if next_url:\n",
        "                current_url = next_url\n",
        "                page_number += 1\n",
        "                # Be respectful with delays\n",
        "                time.sleep(1)\n",
        "            else:\n",
        "                print(\"No more pages found.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nScraping completed! Total books collected: {len(self.books_data)}\")\n",
        "        return self.books_data\n",
        "\n",
        "    def save_to_csv(self, filename=\"books.csv\"):\n",
        "        \"\"\"Save scraped data to CSV file\"\"\"\n",
        "        if not self.books_data:\n",
        "            print(\"No data to save!\")\n",
        "            return\n",
        "\n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(self.books_data)\n",
        "\n",
        "        # Save to CSV\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Data saved to {filename}\")\n",
        "\n",
        "        # Display summary statistics\n",
        "        print(f\"\\n--- Data Summary ---\")\n",
        "        print(f\"Total books: {len(df)}\")\n",
        "        print(f\"Average price: £{df['Price'].mean():.2f}\")\n",
        "        print(f\"Price range: £{df['Price'].min():.2f} - £{df['Price'].max():.2f}\")\n",
        "        print(f\"\\nAvailability distribution:\")\n",
        "        print(df['Availability'].value_counts())\n",
        "        print(f\"\\nStar rating distribution:\")\n",
        "        print(df['Star_Rating'].value_counts().sort_index())\n",
        "\n",
        "        return df\n",
        "\n",
        "    def display_sample_data(self, num_samples=5):\n",
        "        \"\"\"Display sample of scraped data\"\"\"\n",
        "        if not self.books_data:\n",
        "            print(\"No data available!\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.books_data)\n",
        "        print(f\"\\n--- Sample Data (first {num_samples} books) ---\")\n",
        "        print(df.head(num_samples).to_string(index=False))"
      ],
      "metadata": {
        "id": "dWKsw-gG3Mkg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    try:\n",
        "        print(\"=\" * 60)\n",
        "        print(\"BOOKS TO SCRAPE - WEB SCRAPER\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        scraper = BooksScraper()\n",
        "\n",
        "        # DATA COLLECTION PHASE\n",
        "        print(\"Phase 1: Scraping all books from website...\")\n",
        "        books_data = scraper.scrape_all_books()\n",
        "\n",
        "        # DATA PROCESSING AND EXPORT PHASE\n",
        "        if books_data:\n",
        "            print(\"\\nPhase 2: Processing and analyzing data...\")\n",
        "\n",
        "            scraper.display_sample_data()\n",
        "\n",
        "            df = scraper.save_to_csv(\"books.csv\")\n",
        "\n",
        "            # ADVANCED ANALYSIS - Additional insights\n",
        "            print(f\"\\n--- Top 10 Most Expensive Books ---\")\n",
        "            if len(books_data) > 0:\n",
        "                # NLARGEST METHOD - Finds highest values in specified column\n",
        "                df_sorted = df.nlargest(10, 'Price')\n",
        "                # COLUMN SELECTION - Display only relevant columns\n",
        "                print(df_sorted[['Title', 'Price', 'Star_Rating']].to_string(index=False))\n",
        "\n",
        "            # SUCCESS MESSAGE\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"SCRAPING COMPLETED SUCCESSFULLY!\")\n",
        "            print(f\"Total books scraped: {len(books_data)}\")\n",
        "            print(\"Data saved to: books.csv\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "        else:\n",
        "            # FAILURE HANDLING\n",
        "            print(\"ERROR: No books were scraped!\")\n",
        "            print(\"Please check:\")\n",
        "            print(\"1. Internet connection\")\n",
        "            print(\"2. Website accessibility\")\n",
        "            print(\"3. HTML structure changes\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        # GRACEFUL SHUTDOWN - Handle Ctrl+C interruption\n",
        "        print(\"\\nScraping interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        # UNEXPECTED ERROR HANDLING\n",
        "        print(f\"Unexpected error occurred: {e}\")\n",
        "        print(\"Please check your internet connection and try again.\")"
      ],
      "metadata": {
        "id": "UZVjxYeR33EB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs50H96L399n",
        "outputId": "410ba165-f18e-44fd-9e5d-dcd94f402c8e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BOOKS TO SCRAPE - WEB SCRAPER\n",
            "============================================================\n",
            "Phase 1: Scraping all books from website...\n",
            "Starting to scrape all books from Books to Scrape...\n",
            "\n",
            "--- Page 1 ---\n",
            "Scraping: https://books.toscrape.com/\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 2 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-2.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 3 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-3.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 4 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-4.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 5 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-5.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 6 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-6.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 7 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-7.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 8 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-8.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 9 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-9.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 10 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-10.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 11 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-11.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 12 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-12.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 13 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-13.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 14 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-14.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 15 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-15.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 16 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-16.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 17 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-17.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 18 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-18.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 19 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-19.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 20 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-20.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 21 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-21.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 22 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-22.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 23 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-23.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 24 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-24.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 25 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-25.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 26 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-26.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 27 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-27.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 28 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-28.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 29 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-29.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 30 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-30.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 31 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-31.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 32 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-32.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 33 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-33.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 34 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-34.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 35 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-35.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 36 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-36.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 37 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-37.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 38 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-38.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 39 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-39.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 40 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-40.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 41 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-41.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 42 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-42.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 43 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-43.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 44 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-44.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 45 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-45.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 46 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-46.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 47 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-47.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 48 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-48.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 49 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-49.html\n",
            "Found 20 books on this page\n",
            "\n",
            "--- Page 50 ---\n",
            "Scraping: https://books.toscrape.com/catalogue/page-50.html\n",
            "Found 20 books on this page\n",
            "No more pages found.\n",
            "\n",
            "Scraping completed! Total books collected: 1000\n",
            "\n",
            "Phase 2: Processing and analyzing data...\n",
            "\n",
            "--- Sample Data (first 5 books) ---\n",
            "                                Title  Price Availability  Star_Rating\n",
            "                 A Light in the Attic  51.77     In stock            3\n",
            "                   Tipping the Velvet  53.74     In stock            1\n",
            "                           Soumission  50.10     In stock            1\n",
            "                        Sharp Objects  47.82     In stock            4\n",
            "Sapiens: A Brief History of Humankind  54.23     In stock            5\n",
            "Data saved to books.csv\n",
            "\n",
            "--- Data Summary ---\n",
            "Total books: 1000\n",
            "Average price: £35.07\n",
            "Price range: £10.00 - £59.99\n",
            "\n",
            "Availability distribution:\n",
            "Availability\n",
            "In stock    1000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Star rating distribution:\n",
            "Star_Rating\n",
            "1    226\n",
            "2    196\n",
            "3    203\n",
            "4    179\n",
            "5    196\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Top 10 Most Expensive Books ---\n",
            "                                                                                    Title  Price  Star_Rating\n",
            "                                                       The Perfect Play (Play by Play #1)  59.99            3\n",
            "                                                        Last One Home (New Beginnings #1)  59.98            3\n",
            "                                                         Civilization and Its Discontents  59.95            2\n",
            "                                                           The Barefoot Contessa Cookbook  59.92            5\n",
            "                                                                The Diary of a Young Girl  59.90            3\n",
            "                                     The Bone Hunters (Lexy Vaughan & Steven Macaulay #2)  59.71            3\n",
            "Thomas Jefferson and the Tripoli Pirates: The Forgotten War That Changed American History  59.64            1\n",
            "                                                            Boar Island (Anna Pigeon #19)  59.48            3\n",
            "                                                                The Improbability of Love  59.45            1\n",
            "                          The Man Who Mistook His Wife for a Hat and Other Clinical Tales  59.45            4\n",
            "\n",
            "============================================================\n",
            "SCRAPING COMPLETED SUCCESSFULLY!\n",
            "Total books scraped: 1000\n",
            "Data saved to: books.csv\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}